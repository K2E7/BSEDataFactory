{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c48343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend logic for BSE raw JSON dump, importable in a notebook\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from dateutil.parser import parse as dtparse\n",
    "\n",
    "# HTTP headers and API endpoint template\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Origin\": \"https://www.bseindia.com\",\n",
    "    \"Referer\": \"https://www.bseindia.com/\",\n",
    "}\n",
    "URL = (\n",
    "    \"https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w\"\n",
    "    \"?pageno={page}&strCat=-1&strPrevDate={from_}&strScrip={scrip}\"\n",
    "    \"&strSearch=P&strToDate={to_}&strType=C&subcategory=-1\"\n",
    ")\n",
    "\n",
    "\n",
    "def year_chunks(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Yield (start, end) pairs in ~1-year chunks between two dates.\n",
    "    \"\"\"\n",
    "    cur = start_date\n",
    "    one_year = timedelta(days=365)\n",
    "    while cur <= end_date:\n",
    "        nxt = min(cur + one_year - timedelta(days=1), end_date)\n",
    "        yield cur, nxt\n",
    "        cur = nxt + timedelta(days=1)\n",
    "\n",
    "\n",
    "def fetch_chunk(session, scrip, d_from, d_to):\n",
    "    \"\"\"\n",
    "    Retrieve all pages of announcements for a given scrip code\n",
    "    between d_from and d_to (inclusive).\n",
    "    Returns a list of raw JSON payloads.\n",
    "    \"\"\"\n",
    "    data, page = [], 1\n",
    "    while True:\n",
    "        url = URL.format(\n",
    "            page=page,\n",
    "            from_=d_from.strftime(\"%Y%m%d\"),\n",
    "            to_=d_to.strftime(\"%Y%m%d\"),\n",
    "            scrip=scrip,\n",
    "        )\n",
    "        response = session.get(url, headers=HEADERS, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        payload = response.json()\n",
    "        if not payload.get(\"Table\"):\n",
    "            break\n",
    "        data.append(payload)\n",
    "        page += 1\n",
    "    return data\n",
    "\n",
    "def dump_raw_payloads(payloads, outfile):\n",
    "    \"\"\"\n",
    "    Append newline-delimited JSON payloads to the given file path,\n",
    "    each pretty-printed with indentation.\n",
    "    \"\"\"\n",
    "    with outfile.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for p in payloads:\n",
    "            json.dump(p, f, ensure_ascii=False, indent=2)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def run(map_str, start_str, end_str=None, output_dir_name=\"output\"):\n",
    "    \"\"\"\n",
    "    Programmatic entrypoint:\n",
    "      - map_str: comma-separated \"ISIN=scripCode\" pairs\n",
    "      - start_str: YYYY-MM-DD\n",
    "      - end_str: YYYY-MM-DD (defaults to today if None)\n",
    "      - output_dir_name: directory to write JSON files into\n",
    "    Returns a list of log strings.\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start_date = dtparse(start_str).date()\n",
    "    end_date = dtparse(end_str).date() if end_str else datetime.now().date()\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"start date must be <= end date\")\n",
    "\n",
    "    # Parse the mapping string\n",
    "    mapping = {}\n",
    "    for pair in map_str.split(\",\"):\n",
    "        if \"=\" not in pair:\n",
    "            raise ValueError(f\"Invalid mapping entry: '{pair}'\")\n",
    "        isin, scrip = pair.split(\"=\", 1)\n",
    "        mapping[isin.strip()] = scrip.strip()\n",
    "\n",
    "    # Prepare output directory and HTTP session\n",
    "    output_dir = Path(output_dir_name)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    session = requests.Session()\n",
    "    logs = []\n",
    "\n",
    "    # Fetch and dump data in year-long chunks per ISIN\n",
    "    for isin, scrip in mapping.items():\n",
    "        outfile = output_dir / f\"{isin}.json\"\n",
    "        # Overwrite old file if exists\n",
    "        if outfile.exists():\n",
    "            outfile.unlink()\n",
    "        for chunk_start, chunk_end in year_chunks(start_date, end_date):\n",
    "            payloads = fetch_chunk(session, scrip, chunk_start, chunk_end)\n",
    "            dump_raw_payloads(payloads, outfile)\n",
    "            entry = (\n",
    "                f\"{isin} ({scrip}) {chunk_start:%Y-%m-%d}–{chunk_end:%Y-%m-%d} \"\n",
    "                f\"→ {outfile.name}: {len(payloads)} pages\"\n",
    "            )\n",
    "            print(entry)\n",
    "            logs.append(entry)\n",
    "            time.sleep(random.uniform(1, 14))\n",
    "\n",
    "    print(f\"Finished; raw dumps are in ./{output_dir_name}/\")\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1f01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def fetch_bse_code(isins):\n",
    "    url = f\"https://api.bseindia.com/BseIndiaAPI/api/PeerSmartSearch/w?Type=SS&text={isin}\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Referer\": \"https://www.bseindia.com/\",\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=5)\n",
    "    resp.raise_for_status()\n",
    "    m = re.search(r\"liclick\\('(\\d{5,6})'\", resp.text)\n",
    "    return m.group(1) if m else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e66f5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty50_isins = [\n",
    "    \"INE361B01024\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f680dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INE361B01024 → 532488\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "for isin in nifty50_isins:\n",
    "    code = fetch_bse_code(isin)\n",
    "    mapping[isin] = code\n",
    "    print(f\"{isin} → {code}\")\n",
    "    sleep(random.randint(1, 3))\n",
    "\n",
    "# now build your list of \"{isin}={scrip}\" strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17237b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_list = [f\"{isin}={scrip}\" for isin, scrip in mapping.items()]\n",
    "mapping_str = \",\".join(mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4618777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_str = \"2014-01-01\"\n",
    "end_str   = \"2025-01-01\"  # or None to default to today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ed3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INE361B01024=532488\n"
     ]
    }
   ],
   "source": [
    "print(mapping_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4e1fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INE361B01024 (532488) 2014-01-01–2014-12-31 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2015-01-01–2015-12-31 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2016-01-01–2016-12-30 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2016-12-31–2017-12-30 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2017-12-31–2018-12-30 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2018-12-31–2019-12-30 → INE361B01024.json: 2 pages\n",
      "INE361B01024 (532488) 2019-12-31–2020-12-29 → INE361B01024.json: 2 pages\n",
      "INE361B01024 (532488) 2020-12-30–2021-12-29 → INE361B01024.json: 1 pages\n",
      "INE361B01024 (532488) 2021-12-30–2022-12-29 → INE361B01024.json: 2 pages\n",
      "INE361B01024 (532488) 2022-12-30–2023-12-29 → INE361B01024.json: 2 pages\n",
      "INE361B01024 (532488) 2023-12-30–2024-12-28 → INE361B01024.json: 2 pages\n",
      "INE361B01024 (532488) 2024-12-29–2025-01-01 → INE361B01024.json: 1 pages\n",
      "Finished; raw dumps are in ./output/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log = run(mapping_str, start_str, end_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c25220ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONDecoder\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_multiple_json_objects(text):\n",
    "    \"\"\"\n",
    "    Yield each top-level JSON object from a string that contains\n",
    "    multiple JSON objects concatenated together.\n",
    "    \"\"\"\n",
    "    decoder = JSONDecoder()\n",
    "    idx = 0\n",
    "    length = len(text)\n",
    "    while idx < length:\n",
    "        # Skip any whitespace/newlines before the next object\n",
    "        while idx < length and text[idx].isspace():\n",
    "            idx += 1\n",
    "        if idx >= length:\n",
    "            break\n",
    "        # Decode one JSON object starting at idx\n",
    "        obj, end = decoder.raw_decode(text, idx)\n",
    "        yield obj\n",
    "        idx = end\n",
    "\n",
    "def CSVHandler():\n",
    "    input_dir = Path(\"output\")\n",
    "    output_dir = Path(\"csv\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for json_file in input_dir.glob(\"*.json\"):\n",
    "        # Read the entire file as a single string\n",
    "        text = json_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        # Collect all rows from every JSON object’s \"Table\"\n",
    "        rows = []\n",
    "        for obj in parse_multiple_json_objects(text):\n",
    "            table = obj.get(\"Table\")\n",
    "            if isinstance(table, list):\n",
    "                rows.extend(table)\n",
    "\n",
    "        # Build the dataframe with the desired columns\n",
    "        df = pd.DataFrame(rows, columns=[\"HEADLINE\", \"NEWS_DT\", \"SLONGNAME\"])\n",
    "        df.insert(0, \"ISIN\", json_file.stem)\n",
    "\n",
    "        # Write out to csv/<json_filename>.csv\n",
    "        csv_path = output_dir / f\"{json_file.stem}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Wrote {csv_path}\")\n",
    "\n",
    "    print(\"All JSON files processed and converted to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "570ce245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote csv/INE721A01047.csv\n",
      "Wrote csv/INE238A01034.csv\n",
      "Wrote csv/INE066A01021.csv\n",
      "Wrote csv/INE397D01024.csv\n",
      "Wrote csv/INE585B01010.csv\n",
      "Wrote csv/INE062A01020.csv\n",
      "Wrote csv/INE263A01024.csv\n",
      "Wrote csv/INE101A01026.csv\n",
      "Wrote csv/INE009A01021.csv\n",
      "Wrote csv/INE095A01012.csv\n",
      "Wrote csv/INE192A01025.csv\n",
      "Wrote csv/INE795G01014.csv\n",
      "Wrote csv/INE849A01020.csv\n",
      "Wrote csv/INE742F01042.csv\n",
      "Wrote csv/INE437A01024.csv\n",
      "Wrote csv/INE059A01026.csv\n",
      "Wrote csv/INE002A01018.csv\n",
      "Wrote csv/INE752E01010.csv\n",
      "Wrote csv/INE030A01027.csv\n",
      "Wrote csv/INE213A01029.csv\n",
      "Wrote csv/INE423A01024.csv\n",
      "Wrote csv/INE075A01022.csv\n",
      "Wrote csv/INE019A01038.csv\n",
      "Wrote csv/INE044A01036.csv\n",
      "Wrote csv/INE038A01020.csv\n",
      "Wrote csv/INE733E01010.csv\n",
      "Wrote csv/INE361B01024.csv\n",
      "Wrote csv/INE467B01029.csv\n",
      "Wrote csv/INE239A01024.csv\n",
      "Wrote csv/INE155A01022.csv\n",
      "Wrote csv/INE047A01021.csv\n",
      "Wrote csv/INE154A01025.csv\n",
      "Wrote csv/INE918I01026.csv\n",
      "Wrote csv/INE481G01011.csv\n",
      "Wrote csv/INE280A01028.csv\n",
      "Wrote csv/INE040A01034.csv\n",
      "Wrote csv/INE021A01026.csv\n",
      "Wrote csv/INE123W01016.csv\n",
      "Wrote csv/INE089A01031.csv\n",
      "Wrote csv/INE090A01021.csv\n",
      "Wrote csv/INE018A01030.csv\n",
      "Wrote csv/INE860A01027.csv\n",
      "Wrote csv/INE237A01028.csv\n",
      "Wrote csv/INE158A01026.csv\n",
      "Wrote csv/INE296A01024.csv\n",
      "Wrote csv/INE081A01020.csv\n",
      "Wrote csv/INE669C01036.csv\n",
      "Wrote csv/INE917I01010.csv\n",
      "Wrote csv/INE029A01011.csv\n",
      "Wrote csv/INE216A01030.csv\n",
      "Wrote csv/INE522F01014.csv\n",
      "All JSON files processed and converted to CSV.\n"
     ]
    }
   ],
   "source": [
    "CSVHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a6263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
