{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c48343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend logic for BSE raw JSON dump, importable in a notebook\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from dateutil.parser import parse as dtparse\n",
    "\n",
    "# HTTP headers and API endpoint template\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Origin\": \"https://www.bseindia.com\",\n",
    "    \"Referer\": \"https://www.bseindia.com/\",\n",
    "}\n",
    "URL = (\n",
    "    \"https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w\"\n",
    "    \"?pageno={page}&strCat=-1&strPrevDate={from_}&strScrip={scrip}\"\n",
    "    \"&strSearch=P&strToDate={to_}&strType=C&subcategory=-1\"\n",
    ")\n",
    "\n",
    "\n",
    "def year_chunks(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Yield (start, end) pairs in ~1-year chunks between two dates.\n",
    "    \"\"\"\n",
    "    cur = start_date\n",
    "    one_year = timedelta(days=365)\n",
    "    while cur <= end_date:\n",
    "        nxt = min(cur + one_year - timedelta(days=1), end_date)\n",
    "        yield cur, nxt\n",
    "        cur = nxt + timedelta(days=1)\n",
    "\n",
    "\n",
    "def fetch_chunk(session, scrip, d_from, d_to):\n",
    "    \"\"\"\n",
    "    Retrieve all pages of announcements for a given scrip code\n",
    "    between d_from and d_to (inclusive).\n",
    "    Returns a list of raw JSON payloads.\n",
    "    \"\"\"\n",
    "    data, page = [], 1\n",
    "    while True:\n",
    "        url = URL.format(\n",
    "            page=page,\n",
    "            from_=d_from.strftime(\"%Y%m%d\"),\n",
    "            to_=d_to.strftime(\"%Y%m%d\"),\n",
    "            scrip=scrip,\n",
    "        )\n",
    "        response = session.get(url, headers=HEADERS, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        payload = response.json()\n",
    "        if not payload.get(\"Table\"):\n",
    "            break\n",
    "        data.append(payload)\n",
    "        page += 1\n",
    "    return data\n",
    "\n",
    "def dump_raw_payloads(payloads, outfile):\n",
    "    \"\"\"\n",
    "    Append newline-delimited JSON payloads to the given file path,\n",
    "    each pretty-printed with indentation.\n",
    "    \"\"\"\n",
    "    with outfile.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for p in payloads:\n",
    "            json.dump(p, f, ensure_ascii=False, indent=2)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def run(map_str, start_str, end_str=None, output_dir_name=\"output\"):\n",
    "    \"\"\"\n",
    "    Programmatic entrypoint:\n",
    "      - map_str: comma-separated \"ISIN=scripCode\" pairs\n",
    "      - start_str: YYYY-MM-DD\n",
    "      - end_str: YYYY-MM-DD (defaults to today if None)\n",
    "      - output_dir_name: directory to write JSON files into\n",
    "    Returns a list of log strings.\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start_date = dtparse(start_str).date()\n",
    "    end_date = dtparse(end_str).date() if end_str else datetime.now().date()\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"start date must be <= end date\")\n",
    "\n",
    "    # Parse the mapping string\n",
    "    mapping = {}\n",
    "    for pair in map_str.split(\",\"):\n",
    "        if \"=\" not in pair:\n",
    "            raise ValueError(f\"Invalid mapping entry: '{pair}'\")\n",
    "        isin, scrip = pair.split(\"=\", 1)\n",
    "        mapping[isin.strip()] = scrip.strip()\n",
    "\n",
    "    # Prepare output directory and HTTP session\n",
    "    output_dir = Path(output_dir_name)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    session = requests.Session()\n",
    "    logs = []\n",
    "\n",
    "    # Fetch and dump data in year-long chunks per ISIN\n",
    "    for isin, scrip in mapping.items():\n",
    "        outfile = output_dir / f\"{isin}.json\"\n",
    "        # Overwrite old file if exists\n",
    "        if outfile.exists():\n",
    "            outfile.unlink()\n",
    "        for chunk_start, chunk_end in year_chunks(start_date, end_date):\n",
    "            payloads = fetch_chunk(session, scrip, chunk_start, chunk_end)\n",
    "            dump_raw_payloads(payloads, outfile)\n",
    "            entry = (\n",
    "                f\"{isin} ({scrip}) {chunk_start:%Y-%m-%d}–{chunk_end:%Y-%m-%d} \"\n",
    "                f\"→ {outfile.name}: {len(payloads)} pages\"\n",
    "            )\n",
    "            print(entry)\n",
    "            logs.append(entry)\n",
    "            time.sleep(random.uniform(1, 14))\n",
    "\n",
    "    print(f\"Finished; raw dumps are in ./{output_dir_name}/\")\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4618777",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_str   = \"INE002A01018=500325,INE062A01020=500112\"\n",
    "start_str = \"2014-01-01\"\n",
    "end_str   = \"2025-01-01\"  # or None to default to today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e1fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INE002A01018 (500325) 2014-01-01–2014-12-31 → INE002A01018.json: 8 pages\n",
      "INE002A01018 (500325) 2015-01-01–2015-12-31 → INE002A01018.json: 7 pages\n",
      "INE002A01018 (500325) 2016-01-01–2016-12-30 → INE002A01018.json: 2 pages\n",
      "INE002A01018 (500325) 2016-12-31–2017-12-30 → INE002A01018.json: 3 pages\n",
      "INE002A01018 (500325) 2017-12-31–2018-12-30 → INE002A01018.json: 4 pages\n",
      "INE002A01018 (500325) 2018-12-31–2019-12-30 → INE002A01018.json: 6 pages\n",
      "INE002A01018 (500325) 2019-12-31–2020-12-29 → INE002A01018.json: 6 pages\n",
      "INE002A01018 (500325) 2020-12-30–2021-12-29 → INE002A01018.json: 9 pages\n",
      "INE002A01018 (500325) 2021-12-30–2022-12-29 → INE002A01018.json: 9 pages\n",
      "INE002A01018 (500325) 2022-12-30–2023-12-29 → INE002A01018.json: 9 pages\n",
      "INE002A01018 (500325) 2023-12-30–2024-12-28 → INE002A01018.json: 9 pages\n",
      "INE002A01018 (500325) 2024-12-29–2025-01-01 → INE002A01018.json: 1 pages\n",
      "INE062A01020 (500112) 2014-01-01–2014-12-31 → INE062A01020.json: 2 pages\n",
      "INE062A01020 (500112) 2015-01-01–2015-12-31 → INE062A01020.json: 2 pages\n",
      "INE062A01020 (500112) 2016-01-01–2016-12-30 → INE062A01020.json: 3 pages\n",
      "INE062A01020 (500112) 2016-12-31–2017-12-30 → INE062A01020.json: 4 pages\n",
      "INE062A01020 (500112) 2017-12-31–2018-12-30 → INE062A01020.json: 4 pages\n",
      "INE062A01020 (500112) 2018-12-31–2019-12-30 → INE062A01020.json: 4 pages\n",
      "INE062A01020 (500112) 2019-12-31–2020-12-29 → INE062A01020.json: 4 pages\n",
      "INE062A01020 (500112) 2020-12-30–2021-12-29 → INE062A01020.json: 4 pages\n",
      "INE062A01020 (500112) 2021-12-30–2022-12-29 → INE062A01020.json: 6 pages\n",
      "INE062A01020 (500112) 2022-12-30–2023-12-29 → INE062A01020.json: 7 pages\n",
      "INE062A01020 (500112) 2023-12-30–2024-12-28 → INE062A01020.json: 6 pages\n",
      "INE062A01020 (500112) 2024-12-29–2025-01-01 → INE062A01020.json: 1 pages\n",
      "Finished; raw dumps are in ./output/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log = run(map_str, start_str, end_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
